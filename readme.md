# Knowledge Graph 知识图谱

## 基本技能

+ [Windows利用conda命令管理Python环境](https://blog.csdn.net/dengchunlin01/article/details/79075099?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2&utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2)

## KG

+ [知识图谱基础之RDF，RDFS与OWL](https://zhuanlan.zhihu.com/p/32122644)

+ [文因互联鲍捷：深度解析知识图谱发展关键阶段及技术脉络 | 公开课笔记](https://www.jianshu.com/p/54d2cb005397)

## Deep Learning基础

+ [《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记](https://www.cnblogs.com/royhoo/category/1182855.html)

+ [第十四章——循环神经网络（Recurrent Neural Networks）（第一部分）](https://www.cnblogs.com/royhoo/p/Recurrent-Neural-Networks-1.html)

## PyTorch

+ [PyTorch入门教程](https://www.jianshu.com/p/d66319506dd7)

+ [PyTorch 中文手册（pytorch handbook）](https://github.com/zergtant/pytorch-handbook)

## GPU 效率

+ [深度学习PyTorch，TensorFlow中GPU利用率较低，CPU利用率很低，且模型训练速度很慢的问题总结与分析](https://blog.csdn.net/qq_32998593/article/details/92849585?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1)

## Transformers

+ [PyTorch-Transformers:最先进的自然语言处理库(附带python代码)](https://www.jianshu.com/p/e4ce00a41781)

+ [Transformers 简介（上）](https://www.jianshu.com/p/f917f64ff3e3)

+ [Transformers 简介（下）](https://www.jianshu.com/p/d48c45e81483)

+ [Transformers 简介（上）](https://panchuang.net/2020/03/12/transformers-%e7%ae%80%e4%bb%8b%ef%bc%88%e4%b8%8a%ef%bc%89/)

+ [How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&utm_medium=pytorch-transformers-nlp-python)

+ [Transformers 加载预训练模型](https://blog.csdn.net/fendouaini/article/details/105254397)

+ [pytorch的bert预训练模型下载](https://blog.csdn.net/tailonh/article/details/105394010)

### 人工智能遇见磐石
  
  + [Transformers 快速入门 | 一](https://www.jianshu.com/p/8f24b0edcae3)
  
  + [Transformers 词汇表 | 二](https://www.jianshu.com/p/747044689079)
  
  + [Transformers 库常见的用例 | 三](https://www.jianshu.com/p/83feb35a81a3)
  
  + [Transformers 中使用 TorchScript | 四](https://www.jianshu.com/p/8233ce7c78ec)
  
  + [Transformers 模型上传和共享 | 五](https://www.jianshu.com/p/217a40a364b8)
  
  + [Transformers 示例 | 六](https://www.jianshu.com/p/de8755be0230)
  
  + [Transformers 加载预训练模型 | 七](https://www.jianshu.com/p/9f70cf441b22)
  
  + [Transformers 保存并加载模型 | 八](https://www.jianshu.com/p/f044369967aa)
  
  + [Transformers 转换Tensorflow的Checkpoints | 九](https://www.jianshu.com/p/a9874f42f255)
  
  + [Transformers 从pytorch-pretrained-bert迁移 | 十](https://www.jianshu.com/p/9d5f3e7463d3)
  
  + []()
  
  + []()

## BERT

+ [一本读懂BERT(实践篇)](https://blog.csdn.net/jiaowoshouzi/article/details/89388794)

+ [关系抽取](https://blog.csdn.net/weixin_42001089/article/details/97657149)

+ [BERT：代码解读、实体关系抽取实战](https://blog.csdn.net/qq_27586341/article/details/102831870)

+ [利用关系抽取构建知识图谱的一次尝试](https://www.jianshu.com/p/9d33520f2a68)

+ [nlp中实体关系抽取方法总结](https://zhuanlan.zhihu.com/p/77868938)

+ [BERT中的词向量指南，非常的全面，非常的干货](https://blog.csdn.net/u011984148/article/details/99921480)

+ [Pytorch版本的BERT使用学习笔记](https://blog.csdn.net/ccbrid/article/details/88732857)

+ [如何使用BERT实现中文的文本分类（附代码）](https://blog.csdn.net/Real_Brilliant/article/details/84880528)

+ [用sklearn结合bert对中文句子聚类](https://blog.csdn.net/scbl2017/article/details/105379419/)

+ [Pytorch之Bert文本分类（一）](https://blog.csdn.net/SZU_Hadooper/article/details/101692883?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3&utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3)

+ [Pytorch之Bert中文文本分类（二）](https://blog.csdn.net/SZU_Hadooper/article/details/101697580?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-13&utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-13)

+ [一步步理解BERT](http://nlp.dataguru.cn/article-14907-1.html)

+ [基于Bert-NER构建特定领域的中文信息抽取框架（上）](https://www.freebuf.com/column/209195.html)

+ [bert--Ner](https://github.com/FuYanzhe2/Name-Entity-Recognition)

+ [基于BERT fine-tuning的中文标题分类实战](https://zhuanlan.zhihu.com/p/72448986)

## NER
  
+ [命名实体识别NER & 如何使用BERT实现](https://blog.csdn.net/qq_27586341/article/details/103062651?utm_source=distribute.pc_relevant.none-task-blog-baidujs-2)

+ [【最新试验】用预训练模型Roberta做序列标注_自然语言处理_使用RobertaForTokenClassification做命名实体识别pytorch版](https://blog.csdn.net/weixin_39673686/article/details/100939650)

+ [Bert 命名实体识别](https://www.cnblogs.com/jfdwd/p/11238691.html)

+ [基于BERT预训练的中文命名实体识别TensorFlow实现](https://blog.csdn.net/macanv/article/details/85684284)


## KGQA

+ [基于BERT的KBQA探索](https://zhuanlan.zhihu.com/p/62946533)

+ [问题生成 (Question generation)：AI与认知科学](https://zhuanlan.zhihu.com/p/27472012)

+ [awesome-question-answering](https://github.com/dapurv5/awesome-question-answering)

## MRC 机器阅读

+ [机器阅读理解：如何让计算机读懂文章](https://zhuanlan.zhihu.com/p/56981110?from_voters_page=true)

## AIML

+ [AIML框架标签详解](https://blog.csdn.net/weixin_42250835/article/details/87139209)

+ [AIML元素详细说明](https://blog.csdn.net/qq_16633405/article/details/80228697)

+ [AIML知识库数据匹配原理解析](https://blog.csdn.net/qq_16633405/article/details/80479110)

+ [使用Python AIML搭建聊天机器人的方法示例](http://www.python88.cn/art/4570/)

+ [智能客服 对话实现--python aiml包](https://www.cnblogs.com/brucekun/p/7667780.html)

## 精彩博客

+ [天生smile](https://www.jianshu.com/u/abfe703a00fe)

